{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "445c5d6d",
   "metadata": {},
   "source": [
    "# Demo of logits collection and getting entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea6deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import copy\n",
    "import random\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from timm import create_model\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "parser = argparse.ArgumentParser(description='ImageNet Training')\n",
    "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
    "parser.add_argument('--lr_schedule', default=0, type=int, help='lr scheduler')\n",
    "# parser.add_argument('--train_batch', default=512, type=int, help='batch size')\n",
    "parser.add_argument('--valid_batch', default=512, type=int, help='batch size')\n",
    "parser.add_argument('--num_epoch', default=200, type=int, help='epoch number')\n",
    "parser.add_argument('--num_classes', type=int, default=1000, help='number classes')\n",
    "parser.add_argument('--lr_densenet', default=0.1, type=float, help='learning rate')\n",
    "parser.add_argument('--lr_vgg16', default=0.001, type=float, help='learning rate')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "class ImageNet_valid_test(torchvision.datasets.ImageFolder):\n",
    "\n",
    "    def __init__(self, path, transform=None, is_valid=False, num_classes = None):\n",
    "        super(ImageNet_valid_test, self).__init__(root=path, transform=transform) \n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "        self.is_valid = is_valid\n",
    "\n",
    "        if self.is_valid:\n",
    "            data_for_valid = []\n",
    "            for i in range(num_classes):\n",
    "\n",
    "                data_per_class = self.samples[(0+50*i) : (25+50*i)]\n",
    "                data_for_valid.extend(data_per_class)\n",
    "            self.samples = data_for_valid\n",
    "\n",
    "        else:\n",
    "            data_for_test = []\n",
    "            for i in range(num_classes):\n",
    "                test_data_per_class = self.samples[(25+50*i) : (50+50*i)]\n",
    "                data_for_test.extend(test_data_per_class) \n",
    "            self.samples = data_for_test\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "    \n",
    "def valid(model, testloader, T=1, device=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    logits_list = []\n",
    "    labels_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            logits_list.append(outputs/T)\n",
    "            labels_list.append(targets)\n",
    "\n",
    "        logits = torch.cat(logits_list).cpu().numpy()\n",
    "        labels = torch.cat(labels_list).cpu().numpy()\n",
    "        ece = tfp.stats.expected_calibration_error(10, logits=logits, labels_true=labels, labels_predicted=np.argmax(logits,1))\n",
    "    return correct / total, logits, labels, ece\n",
    "\n",
    "def ece_eval(preds, targets, n_bins=15, bg_cls = -1):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    confidences, predictions = np.max(preds,1), np.argmax(preds,1)#confidences: pred prob; predictions: pred classes\n",
    "    confidences, predictions = confidences[targets>bg_cls], predictions[targets>bg_cls]#len: 10000\n",
    "    accuracies = (predictions == targets[targets>bg_cls]) \n",
    "    \n",
    "    Bm, acc, conf = np.zeros(n_bins), np.zeros(n_bins), np.zeros(n_bins)\n",
    "    ece = 0.0\n",
    "    bin_idx = 0\n",
    "   \n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = np.logical_and(confidences > bin_lower, confidences <= bin_upper)#boolean vector of len 100\n",
    "        bin_size = np.sum(in_bin)\n",
    "        Bm[bin_idx] = bin_size\n",
    "        if bin_size > 0:  \n",
    "            accuracy_in_bin = np.sum(accuracies[in_bin])\n",
    "            acc[bin_idx] = accuracy_in_bin / Bm[bin_idx]\n",
    "            confidence_in_bin = np.sum(confidences[in_bin])\n",
    "            conf[bin_idx] = confidence_in_bin / Bm[bin_idx]\n",
    "\n",
    "        bin_idx += 1\n",
    "    ece = (Bm * np.abs((acc - conf))).sum()/ Bm.sum()\n",
    "    ece_level = (Bm * (conf - acc)).sum()/ Bm.sum()\n",
    "    return ece, acc, conf, Bm, ece_level\n",
    "\n",
    "def optimal_T(logits, labels, upper=None, lower=None):\n",
    "    best_ece = np.inf\n",
    "    best_t = 0\n",
    "    for T in np.arange(lower, upper, 0.01):\n",
    "        logits = torch.tensor(logits_bs/T)\n",
    "        logits_all =F.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "        ece,_,_,_,_ = ece_eval(logits_all, labels)\n",
    "        if ece < best_ece:\n",
    "            best_ece = ece\n",
    "            best_t = T\n",
    "    return np.round(best_ece,3), best_t\n",
    "\n",
    "    \n",
    "def classwise_ece(logits, labels, num_cls):\n",
    "    ece_per_class = []\n",
    "    ece_level_per_class = []\n",
    "    for i in range (num_cls):\n",
    "        ece_c, acc_c, conf_c, Bm_c, level_c = ece_eval(logits[labels==i], labels[labels==i])\n",
    "        ece_per_class.append(np.round(ece_c,3))\n",
    "\n",
    "        ece_level_per_class.append(np.round(level_c,3))\n",
    "\n",
    "    return np.array(ece_per_class).mean(), np.array(ece_level_per_class).mean(), ece_level_per_class\n",
    "\n",
    "def tuning_ece_level_factor(opt_t, norm_ece_level,model, cifar10_test_loader,device):\n",
    "    \n",
    "    best_ece, best_ece_level_factor,best_ece_level_per_class = np.inf, 0, 0\n",
    "    \n",
    "    for i, ece_level_factor in enumerate(np.arange(-2, -2, 0.1)):\n",
    "        ece_level_t = opt_t + norm_ece_level * ece_level_factor\n",
    "        ece_level_t = torch.tensor(ece_level_t).float().to(device)\n",
    "        accuracy, logits, labels,_ = valid(model, cifar10_test_loader, T = ece_level_t, device = device)\n",
    "\n",
    "        logits = torch.tensor(logits)\n",
    "        logits_all = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "        \n",
    "        ece_c, acc_c, conf_c, Bm_c, diff_c = ece_eval(logits_all, labels)\n",
    "\n",
    "        if ece_c<best_ece:\n",
    "            best_ece = ece_c\n",
    "            best_ece_level_factor = ece_level_factor\n",
    "            best_ece_level_per_class=ece_level_t\n",
    "    print(\"best ece and the ece level factor:\",best_ece, best_ece_level_factor)\n",
    "    return best_ece_level_per_class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a3e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user-data/sa25729/myenv1/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/user-data/sa25729/myenv1/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "valdir = os.path.join('imagenet', 'val')\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# transform_val = transforms.Compose([transforms.Resize(256),transforms.CenterCrop(224),\n",
    "#         transforms.ToTensor(),normalize])\n",
    "transform_test = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), normalize])\n",
    "\n",
    "valid_dataset = ImageNet_valid_test(os.path.join(valdir),\n",
    "            transform=transform_test, is_valid = True, num_classes= args.num_classes)\n",
    "\n",
    "test_dataset = ImageNet_valid_test(os.path.join(valdir),transform = transform_test, \n",
    "                                   is_valid = False, num_classes= args.num_classes)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=args.valid_batch, shuffle=False,\n",
    "                                          num_workers=2, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.valid_batch, shuffle=False, \n",
    "                                         num_workers=2, pin_memory=True)\n",
    "\n",
    "model = models.resnet34(pretrained=True).to(device)\n",
    "model = model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=False, weight_decay=5e-4)\n",
    "\n",
    "# model = models.densenet121(pretrained=True).to(device)\n",
    "# model = model.to(device)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=args.lr_densenet, momentum=0.9, nesterov=False, weight_decay=5e-4)\n",
    "\n",
    "# model = models.vgg16(pretrained=True).to(device)\n",
    "# model = model.to(device)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=args.lr_vgg16, momentum=0.9, nesterov=False, weight_decay=5e-4)\n",
    "\n",
    "# efficientnet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\n",
    "# model = efficientnet.eval().to(device)\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# vit = create_model(\"vit_large_patch16_224\", pretrained=True).to(device)#vit_base_patch16_224\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edd72",
   "metadata": {},
   "source": [
    "Logit Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f5847f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 12:25:29.551843: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-09 12:25:30.408575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-06-09 12:25:34.099715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 16432 MB memory:  -> device: 0, name: Tesla P40, pci bus id: 0000:3b:00.0, compute capability: 6.1\n",
      "2023-06-09 12:25:34.100428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 21794 MB memory:  -> device: 1, name: Tesla P40, pci bus id: 0000:af:00.0, compute capability: 6.1\n",
      "/data/user-data/sa25729/myenv1/lib/python3.8/site-packages/numpy/lib/npyio.py:521: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.asanyarray(arr)\n",
      "/data/user-data/sa25729/myenv1/lib/python3.8/site-packages/numpy/lib/npyio.py:521: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    }
   ],
   "source": [
    "accuracy, logits, labels, _ = valid(model, test_loader)\n",
    "logits = torch.tensor(logits)\n",
    "np.save('logits_sample.npy', (logits, labels), allow_pickle=True, fix_imports=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e74eaa5",
   "metadata": {},
   "source": [
    "Get the entropy of each sample based on the logit, then rank samples accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d63e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1647440/2078218413.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  logits_bs_ = torch.tensor(logits_bs)\n",
      "/tmp/ipykernel_1647440/2078218413.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  logits = torch.tensor(logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minmax entropy: tensor(1.2066e-07) tensor(22968) tensor(5.3163) tensor(20630)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "def entropy_calculation(logits):\n",
    "    logits = torch.tensor(logits)\n",
    "    sm_logits_all = []\n",
    "    etp_all = []\n",
    "    for i in range(len(logits)):\n",
    "        sm_logits = F.softmax(logits[i], dim = 0)\n",
    "        etp = Categorical(probs = sm_logits).entropy()\n",
    "        sm_logits_all.extend(sm_logits)\n",
    "        etp_all.append(etp)\n",
    "    \n",
    "    return sm_logits_all, etp_all\n",
    "\n",
    "logits_bs, labels_bs = np.load('logits_sample.npy',allow_pickle=True)\n",
    "\n",
    "\n",
    "logits_bs_ = torch.tensor(logits_bs)\n",
    "# labels_bs_ = torch.tensor(labels_bs).long()\n",
    "\n",
    "conf, entropy = entropy_calculation(logits_bs)\n",
    "entropy_ = torch.tensor(entropy)\n",
    "entropy_sort = torch.sort(entropy_)\n",
    "print(\"minmax entropy:\",entropy_sort[0][0], entropy_sort[1][0], entropy_sort[0][-1], entropy_sort[1][-1])\n",
    "\n",
    "np.save('entropy_sample.npy', (logits_bs, labels_bs, entropy, \n",
    "                                          np.array(entropy_sort[0]), np.array(entropy_sort[1])), \n",
    "                                          allow_pickle=True, fix_imports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0191ba95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
